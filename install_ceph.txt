#cek partisi
pastikan tidak ada disk label dos, jika ada silahkan format dengan cara 
- fdisk
- gdisk

#set hostname
sudo hostnamectl set-hostname 03-zulu
sudo vim /etc/hosts
sudo reboot

edit hosts
# Ceph nodes
10.22.32.151 01-zulu
10.22.32.152 02-zulu
10.22.32.153 03-zulu
10.22.32.154 04-zulu
10.22.32.155 05-zulu

#create user cephadmin or you can using root with ssh key

sudo useradd -d /home/cephadmin -m cephadmin -s /bin/bash && sudo passwd cephadmin
echo "cephadmin ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephadmin && sudo chmod 0440 /etc/sudoers.d/cephadmin

#Install nn and other basic utilities:

sudo apt update
sudo apt -y install software-properties-common git curl vim bash-completion ansible

#Confirm Ansible has been installed.


$ ansible --version
ansible 2.9.6
  config file = /etc/ansible/ansible.cfg
  configured module search path = ['/root/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']
  ansible python module location = /usr/lib/python3/dist-packages/ansible
  executable location = /usr/bin/ansible
  python version = 3.8.2 (default, Apr 27 2020, 15:53:34) [GCC 9.3.0]


#Ensure /usr/local/bin path is added to PATH.

echo "PATH=\$PATH:/usr/local/bin" >>~/.bashrc
source ~/.bashrc

#Check your current PATH:

$ echo $PATH
/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/bin

#create hosts ansible

vim /etc/ansible/hosts
[mons]
01-zulu
02-zulu
03-zulu
04-zulu
05-zulu

#/etc/ansible/ansible.cfg and uncomment the inventory

#create sshkey at all hosts
ssh-keygen -t rsa -b 4096 -N '' -f ~/.ssh/id_rsa

#allow root login /etc/ssh/sshd_config
uncomment 
PermitRootLogin yes

#restart sshd
systemctl restart sshd


#verify all hosts connected
ansible all -m ping

#Install Cephadm:

curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm
chmod +x cephadm
sudo mv cephadm  /usr/local/bin/

#verify cephadmin
cephadm --help


#Step 2: Update all Ceph nodes
cd ~/
vim prepare-ceph-nodes.yml

#Modify below contents to set correct timezone and add to the file.

---
- name: Prepare ceph nodes
  hosts: ceph_nodes
  become: yes
  become_user: root
  tasks:
    - name: Set timezone
      timezone:
        name: Asia/Jakarta

    - name: Update system
      apt:
        name: "*"
        state: latest
        update_cache: yes

    - name: Install common packages
      apt:
        name: [vim,git,bash-completion,wget,curl,chrony]
        state: present
        update_cache: yes

    - name: Set authorized key taken from file to root user
      authorized_key:
        user: root
        state: present
        key: "{{ lookup('file', '~/.ssh/id_rsa.pub') }}"

    - name: Install Docker
      shell: |
        curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
        echo "deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable" > /etc/apt/sources.list.d/docker-ce.list
        apt update
        apt install -qq -y docker-ce docker-ce-cli containerd.io
    - name: Reboot server after update and configs
      reboot:

Catatan: reboot is optional

#Save key passphrase if you use one.

$ eval `ssh-agent -s` && ssh-add ~/.ssh/id_rsa
Agent pid 3275
Enter passphrase for /root/.ssh/id_rsa_jmutai: 
Identity added: /root/.ssh/id_rsa_jkmutai (/root/.ssh/id_rsa_jmutai)

#Configure ssh:

tee -a ~/.ssh/config<<EOF
Host *
    UserKnownHostsFile /dev/null
    StrictHostKeyChecking no
    IdentitiesOnly yes
    ConnectTimeout 0
    ServerAliveInterval 300
EOF

#Execute Playbook:

ansible-playbook -i hosts prepare-ceph-nodes.yml --user root

#optional

Configure /etc/hosts
Update /etc/hosts on all nodes if you don’t active DNS configured for hostnames on all cluster servers.

Here is the playbook to modify:

# more update-hosts.yml
---
- name: Prepare ceph nodes
  hosts: ceph_nodes
  become: yes
  become_user: root
  tasks:
    - name: Clean /etc/hosts file
      copy:
        content: ""
        dest: /etc/hosts

    - name: Update /etc/hosts file
      blockinfile:
        path: /etc/hosts
        block: |
           127.0.0.1     localhost
           10.22.64.151  01-zulu
           10.22.64.152  02-zulu
           10.22.64.153  03-zulu
           10.22.64.154  04-zulu
           10.22.64.155  05-zulu
	   
	   
$ ansible-playbook -i hosts update-hosts.yml --user root
		   
#Deploy Ceph 15 (Octopus) Storage Cluster on Ubuntu 20.04	

----
(optional) if reinstall, please doing this
sudo rm /etc/apt/sources.list.d/ceph.list
sudo rm /etc/apt/trusted.gpg.d/ceph.release.gpg
sudo /usr/local/bin/cephadm add-repo --release octopus
sudo /usr/local/bin/cephadm install
if [[ ! -d "/etc/ceph" ]]; then sudo mkdir -p /etc/ceph;fi
--

cat <<EOF > /root/ceph.conf
[global]
public network = 10.22.64.0/24
cluster network = 10.22.80.0/24
EOF


sudo mkdir -p /etc/ceph
cephadm bootstrap -c /root/ceph.conf \
  --mon-ip 10.22.64.151 \
  --initial-dashboard-user ninjafs \
  --initial-dashboard-password arielgantengbangetdeh \
  --dashboard-password-noupdate 

#Install Ceph tools.

cephadm add-repo --release octopus
cephadm install ceph-common

#Ceph nodes add via orch(orchestrator)
root@01-zulu:~# sudo ceph orch host add 02-zulu
Added host '02-zulu'
root@01-zulu:~# sudo ceph orch host add 03-zulu
Added host '03-zulu'
root@01-zulu:~# sudo ceph orch host add 04-zulu
Added host '04-zulu'
root@01-zulu:~# sudo ceph orch host add 05-zulu

#validation

#show config
$  ceph config get mgr

#show list host
$ ceph orch host ls


#Creating an erasure-coded pool (referensi: Mastering ceph)

#To see a list of the erasure profiles, run the following command:
$ ceph osd erasure-code-profile ls

#Let's see what configuration options it contains using the following command:
$ ceph osd erasure-code-profile get default

#This is almost perfect for our test cluster; however, for the purpose of this exercise, we will
create a new profile using the following commands:
$ ceph osd erasure-code-profile set example_profile k=2 m=1 plugin=jerasure technique=reed_sol_van
$ ceph osd erasure-code-profile ls

#You can see our new example_profile has been created:
Now, let's create our erasure-coded pool with this profile:
# ceph osd pool create ecpool 128 128 erasure example_profile

#The preceding command gives the following output:
The preceding command instructs Ceph to create a new pool called ecpool with 128 PGs.
It should be an erasure-coded pool and should use the "example_profile"  we previously
created.
Let's create an object with a small text string inside it and then prove the data has been
stored by reading it back:
$ echo "I am test data for a test object" | rados --pool
ecpool put Test1 –
$ rados --pool ecpool get Test1 -

#First, find out what PG is holding the object we just created:
$ ceph osd map ecpool Test1

#We can now look at the folder structure of the OSDs and see how the object has been split
using the following commands:
$ ls -l /var/lib/ceph/osd/ceph-2/current/1.40s0_head/
# The preceding command gives the following output:
$ ls -l /var/lib/ceph/osd/ceph-1/current/1.40s1_head/
# The preceding command gives the following output:
$ ls -l /var/lib/ceph/osd/ceph-0/current/1.40s2_head/


#Troubleshooting

**
[WARNING]: Consider using the get_url or uri module rather than running 'curl'.  If you need to use command because get_url or uri is
insufficient you can add 'warn: false' to this command task or set 'command_warnings=False' in ansible.cfg to get rid of this message.

command_warnings=False

**
#install monmaptool 
apt install ceph-base -y

#uninstall ceph & docker
sudo apt-get remove ceph* && sudo apt-get autoremove ceph* && sudo apt-get purge ceph* && sudo apt-get autoremove --purge ceph*
sudo rm -fr /etc/systemd/system/ceph* && rm -fr /etc/systemd/system/multi-user.target.wants/ceph* && rm -fr /etc/logrotate.d/ceph* && rm -fr /var/log/ceph

sudo apt-get purge -y docker-engine docker docker.io docker-ce docker-ce-cli && sudo apt-get autoremove -y --purge docker-engine docker docker.io docker-ce  
sudo rm -rf /var/lib/docker /etc/docker && sudo groupdel docker && sudo rm -rf /var/run/docker.sock && sudo rm /etc/apparmor.d/docker 

